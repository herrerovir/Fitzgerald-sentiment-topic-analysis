{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9733b31b",
   "metadata": {},
   "source": [
    "# Discovering Themes\n",
    "### Uncovering Latent Topics in Fitzgerald’s Novels with NLP\n",
    "\n",
    "This notebook explores recurring themes in F. Scott Fitzgerald’s novels using topic modeling. By applying unsupervised learning methods like Latent Dirichlet Allocation (LDA) and BERTopic, it looks for underlying patterns and semantic structures in the text. The aim is to highlight dominant themes and track how they change across chapters or different works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4885f8",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e97538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "%run ../notebooks/setup_path.py\n",
    "from config import *\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Topic Modeling\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import TfidfModel, LdaModel, LdaMulticore, CoherenceModel\n",
    "\n",
    "# Utilities\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35940f33",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "\n",
    "This part of the project focuses on preparing the cleaned, sentence-tokenized texts for topic modeling. Since the texts are already split into sentences, the next step is to apply word tokenization, part-of-speech tagging, and lemmatization at the sentence level. This helps normalize words to their base forms while keeping their grammatical context, which improves the clarity and relevance of the topics. Once each sentence is processed, the lemmatized tokens are grouped back by chapters based on previously identified chapter boundaries. This method preserves the thematic flow within chapters, allowing topic modeling to capture how themes shift and develop throughout each novel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01536f25",
   "metadata": {},
   "source": [
    "- **Stopwords**\n",
    "\n",
    "Standard English stopwords are loaded and expanded with custom stopwords, including frequent character names and locations from the novels. This helps prevent these terms from dominating the topic modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22544489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d464dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = [\n",
    "    # Character Names\n",
    "    \"amory\", \"blaine\", \"isabelle\", \"borgé\", \"rosalind\", \"connage\", \"eleanor\", \"savage\", \"beatrice\", \"monsignor\",\n",
    "    \"darcy\", \"thayer\", \"burne\", \"anthony\", \"patch\", \"gloria\", \"gilbert\", \"caramel\", \"dick\", \"richard\",\n",
    "    \"maury\", \"noble\", \"joseph\", \"bloeckman\", \"muriel\", \"nick\", \"carraway\", \"gatsby\", \"jay\", \"james\",\n",
    "    \"gatz\", \"daisy\", \"tom\", \"buchanan\", \"jordan\", \"baker\", \"myrtle\", \"george\", \"wilson\", \"owl\",\n",
    "    \"eyes\", \"klipspringer\", \"wolfsheim\", \"meyer\", \"dan\", \"cody\", \"henry\", \"catherine\", \"pammy\", \"mckee\",\n",
    "    \"michaelis\", \"alec\", \"old\", \"sport\", \"sally\", \"marietta\", \"wolfshiem\", \"adam\", \"barbara\", \"rachael\",\n",
    "    \"kane\",\n",
    "\n",
    "    # Places\n",
    "    \"princeton\", \"minneapolis\", \"atlantic\", \"new\", \"york\", \"harvard\", \"broadway\", \"west\", \"egg\", \"east\",\n",
    "    \"valley\", \"ashes\", \"fifth\", \"avenue\", \"long\", \"island\", \"midwest\", \"chicago\", \"garage\", \"pennsylvania\",\n",
    "\n",
    "    # Social Titles and Misc. Proper Nouns\n",
    "    \"baedeker\", \"captain\", \"charles\", \"clara\", \"ella\", \"eckleburg\", \"eyebrow\", \"granny\", \"janitor\", \"john\",\n",
    "    \"kerry\", \"mary\", \"miss\", \"mister\", \"mr\", \"mrs\", \"robert\", \"sloane\", \"thomas\", \"william\",\n",
    "    \"regis\", \"geraldine\", \"paramore\", \"grandfather\", \"officer\", \"rose\"\n",
    "]\n",
    "\n",
    "stop_words = stop_words.union(custom_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3038a9",
   "metadata": {},
   "source": [
    "Additional words are added to the stopword list after the initial model run. These come from reviewing the most frequent words and top topic words that didn’t describe the themes but were part of the narrative, ensuring the model focuses on meaningful topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa02583",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = [\n",
    "    # Generic Verbs\n",
    "    \"asked\", \"became\", \"called\", \"came\", \"come\", \"cried\", \"decided\", \"dozen\", \"feel\", \"find\",\n",
    "    \"found\", \"get\", \"give\", \"go\", \"going\", \"got\", \"gone\", \"heard\", \"knew\", \"let\",\n",
    "    \"look\", \"looked\", \"made\", \"make\", \"met\", \"moved\", \"put\", \"read\", \"said\", \"sat\",\n",
    "    \"saw\", \"say\", \"see\", \"set\", \"started\", \"stood\", \"suppose\", \"take\", \"talk\", \"tell\",\n",
    "    \"think\", \"told\", \"took\", \"tried\", \"trying\", \"turned\", \"used\", \"walked\", \"want\", \"went\",\n",
    "    \"seemed\", \"believe\", \"remembered\", \"demanded\", \"suggested\", \"hear\", \"call\", \"answered\", \"write\", \"getting\",\n",
    "    \"commenced\", \"cutting\", \"chewing\", \"waving\", \"humming\", \"know\", \"looking\", \"began\", \"seen\", \"passed\",\n",
    "\n",
    "    # Adjectives & Modifiers\n",
    "    \"almost\", \"alone\", \"big\", \"blue\", \"better\", \"cold\", \"dark\", \"enough\", \"ever\", \"full\",\n",
    "    \"good\", \"gray\", \"great\", \"high\", \"hot\", \"late\", \"little\", \"many\", \"much\", \"next\",\n",
    "    \"quite\", \"really\", \"rather\", \"short\", \"small\", \"still\", \"sure\", \"white\", \"young\", \"whole\",\n",
    "    \"certain\", \"far\", \"often\", \"always\", \"never\", \"several\", \"few\", \"fewer\", \"even\", \"suddenly\",\n",
    "    \"right\", \"left\", \"perhaps\", \"else\", \"well\", \"something\", \n",
    "\n",
    "    # Common Nouns\n",
    "    \"air\", \"arm\", \"back\", \"bed\", \"book\", \"car\", \"corner\", \"door\", \"start\", \"end\",\n",
    "    \"face\", \"foot\", \"front\", \"girl\", \"hand\", \"head\", \"home\", \"house\", \"light\", \"life\",\n",
    "    \"name\", \"part\", \"people\", \"place\", \"room\", \"side\", \"sound\", \"street\", \"table\", \"thing\",\n",
    "    \"time\", \"window\", \"woman\", \"word\", \"world\", \"hair\", \"apartment\", \"chair\", \"glass\", \"floor\",\n",
    "    \"school\", \"college\", \"hall\", \"road\", \"train\", \"town\", \"picture\", \"way\", \"class\", \"man\",\n",
    "    \"men\",\n",
    "\n",
    "    # Weak Concepts\n",
    "    \"anything\", \"clad\", \"considered\", \"continued\", \"done\", \"everything\", \"fact\", \"felt\", \"followed\", \"matter\",\n",
    "    \"mean\", \"nothing\", \"question\", \"sense\", \"story\", \"thought\", \"wanted\", \"yet\", \"course\", \"chapter\",\n",
    "    \"outline\", \"pass\", \"voice\", \"sort\",\n",
    "\n",
    "    # Setting Descriptors\n",
    "    \"morning\", \"afternoon\", \"evening\", \"night\", \"open\", \"closed\", \"day\", \"week\", \"month\", \"year\",\n",
    "    \"minute\", \"minutes\", \"hour\", \"hours\", \"moment\", \"later\", \"second\", \"seconds\", \"first\", \"last\",\n",
    "\n",
    "    # Narration Fillers\n",
    "    \"idea\", \"oh\", \"yes\", \"no\",\n",
    "\n",
    "    # Miscellaneous\n",
    "    \"butler\", \"dot\", \"someone\", \"wreath\", \"pillar\", \"bench\", \"pump\", \"cake\", \"motor\", \"incredulously\",\n",
    "    \"telegram\", \"tapped\", \"violence\", \"impassioned\", \"desolate\", \"soggy\", \"groaning\", \"cellar\", \"invented\", \"lethargic\",\n",
    "    \"tray\", \"brick\", \"gloomy\", \"reappeared\", \"closing\", \"visible\", \"supercilious\", \"deeper\", \"pointing\", \"freshman\",\n",
    "    \"basket\", \"celebrated\", \"raising\", \"borrowed\", \"perceptible\", \"touching\", \"rejected\", \"carelessly\", \"gust\", \"enchanted\",\n",
    "    \"practice\", \"nodding\", \"squeezed\"\n",
    "]\n",
    "\n",
    "stop_words = stop_words.union(additional_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96205354",
   "metadata": {},
   "source": [
    "- **Helper Functions**\n",
    "\n",
    "Two functions support the text processing workflow: one splits the full book into chapters, and the other processes individual sentences by tokenizing, lemmatizing, removing stopwords, and filtering by part of speech. The split_into_chapters function was originally created in the Jupyter notebook 02-sentiment-analysis-fitzgerald.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b32775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chapters(text):\n",
    "    \"\"\"\n",
    "    Splits a full text into chapters based on common chapter headings.\n",
    "\n",
    "    Recognizes patterns like \"Chapter 1\", \"Chapter I\" etc.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        text (str): The full raw text of a book or document.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "        List[Tuple[str, str]]: A list of tuples, each containing:\n",
    "            - chapter title\n",
    "            - chapter text\n",
    "    \"\"\"\n",
    "    chapter_pattern = re.compile(r\"(?:^|\\n)(chapter [ivxlc\\d]+|chapter \\w+)\", re.IGNORECASE)\n",
    "    splits = chapter_pattern.split(text)\n",
    "    chapters = []\n",
    "    for i in range(1, len(splits), 2):\n",
    "        chapter_title = splits[i].strip()\n",
    "        chapter_text = splits[i+1] if i+1 < len(splits) else \"\"\n",
    "        chapters.append((chapter_title, chapter_text.strip()))\n",
    "    return chapters\n",
    "\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Processes a sentence by tokenizing, lemmatizing, removing stopwords, and filtering by part of speech.\n",
    "\n",
    "    Keeps only nouns, verbs, adjectives, and adverbs. Returns a list of cleaned, lowercased lemmas.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        sentence (str): A raw sentence or string of text to be processed.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "        List[str]: A list of lemmatized, lowercased words (lemmas) that:\n",
    "            - are alphabetic\n",
    "            - are not stopwords\n",
    "            - are tagged as noun, verb, adjective, or adverb\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha()]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    lemmas = []\n",
    "    for token, tag in tagged:\n",
    "        if tag.startswith((\"NN\", \"JJ\", \"VB\", \"RB\")):\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            if lemma not in stop_words:\n",
    "                lemmas.append(lemma)\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4cb96",
   "metadata": {},
   "source": [
    "- **Book Processing**\n",
    "\n",
    "Each book file is loaded, split into chapters, and processed one chapter at a time. Sentences are cleaned and tokenized using helper functions, producing lists of relevant lemmas for every chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2bd44f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Topic Modeling Processing ==\n",
      "\n",
      "Processing 'This Side Of Paradise': 9 chapters found\n",
      "Processing completed for 'This Side Of Paradise'.\n",
      "\n",
      "Processing 'The Beautiful And Damned': 9 chapters found\n",
      "Processing completed for 'The Beautiful And Damned'.\n",
      "\n",
      "Processing 'The Great Gatsby': 9 chapters found\n",
      "Processing completed for 'The Great Gatsby'.\n"
     ]
    }
   ],
   "source": [
    "print(\"== Topic Modeling Processing ==\")\n",
    "\n",
    "book_chapter_tokens = defaultdict(list)\n",
    "\n",
    "for title in BOOKS:\n",
    "    clean_path = PROCESSED_DIR / f\"{title}-cleaned.txt\"\n",
    "    if not clean_path.exists():\n",
    "        print(f\"File not found: {clean_path}\")\n",
    "        continue\n",
    "\n",
    "    with open(clean_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    chapters = split_into_chapters(text)\n",
    "\n",
    "    print(f\"\\nProcessing '{title.replace('-', ' ').title()}': {len(chapters)} chapters found\")\n",
    "\n",
    "    for chapter_title, chapter_text in chapters:\n",
    "        sentences = chapter_text.split(\"\\n\")\n",
    "        chapter_tokens = []\n",
    "        for sentence in sentences:\n",
    "            lemmas = process_sentence(sentence)\n",
    "            chapter_tokens.extend(lemmas)\n",
    "\n",
    "        book_chapter_tokens[title].append({\n",
    "            \"chapter_title\": chapter_title,\n",
    "            \"tokens\": chapter_tokens\n",
    "        })\n",
    "\n",
    "    print(f\"Processing completed for '{title.replace('-', ' ').title()}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfc38e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Processed Book Summary ==\n",
      "\n",
      "Total processed books: 3\n",
      "----------------------------------------\n",
      "\n",
      "Title: 'This Side Of Paradise'\n",
      "\n",
      " Chapter 1: 3430 tokens\n",
      " First 10 tokens: ['inherited', 'mother', 'trait', 'stray', 'inexpressible', 'worth', 'father', 'ineffectual', 'inarticulate', 'taste']\n",
      "\n",
      " Chapter 2: 5297 tokens\n",
      " First 10 tokens: ['spire', 'gargoyle', 'noticed', 'wealth', 'sunshine', 'creeping', 'green', 'sward', 'dancing', 'leaded']\n",
      "\n",
      " Chapter 3: 2603 tokens\n",
      " First 10 tokens: ['egotist', 'considers', 'ouch', 'dropped', 'shirt', 'studit', 'hurt', 'melook', 'neck', 'spot']\n",
      "\n",
      " Chapter 4: 3721 tokens\n",
      " First 10 tokens: ['narcissus', 'duty', 'transition', 'period', 'change', 'broaden', 'live', 'gothic', 'beauty', 'parade']\n",
      "\n",
      " Chapter 5: 2210 tokens\n",
      " First 10 tokens: ['debutante', 'february', 'large', 'dainty', 'bedroom', 'sixtyeighth', 'pink', 'wall', 'curtain', 'pink']\n",
      "\n",
      " Chapter 6: 2192 tokens\n",
      " First 10 tokens: ['experiment', 'convalescence', 'knickerbocker', 'bar', 'beamed', 'maxfield', 'parrish', 'jovial', 'colorful', 'king']\n",
      "\n",
      " Chapter 7: 1971 tokens\n",
      " First 10 tokens: ['irony', 'afterward', 'sobbing', 'sending', 'chill', 'beside', 'heart', 'rode', 'slope', 'watched']\n",
      "\n",
      " Chapter 8: 921 tokens\n",
      " First 10 tokens: ['sacrifice', 'city', 'paced', 'board', 'walk', 'lulled', 'everlasting', 'surge', 'changing', 'wave']\n",
      "\n",
      " Chapter 9: 2955 tokens\n",
      " First 10 tokens: ['egotist', 'becomes', 'personage', 'fathom', 'deep', 'sleep', 'lie', 'desire', 'restrained', 'clamor']\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Title: 'The Beautiful And Damned'\n",
      "\n",
      " Chapter 1: 2597 tokens\n",
      " First 10 tokens: ['twentyfive', 'already', 'irony', 'holy', 'ghost', 'theoretically', 'least', 'descended', 'upon', 'irony']\n",
      "\n",
      " Chapter 2: 3630 tokens\n",
      " First 10 tokens: ['portrait', 'siren', 'crispness', 'folded', 'bringing', 'november', 'football', 'game', 'fluttering', 'fur']\n",
      "\n",
      " Chapter 3: 4757 tokens\n",
      " First 10 tokens: ['connoisseur', 'kiss', 'undergraduate', 'editor', 'crimson', 'desired', 'senior', 'picked', 'glorified', 'illusion']\n",
      "\n",
      " Chapter 4: 5142 tokens\n",
      " First 10 tokens: ['radiant', 'fortnight', 'indulge', 'practical', 'discussion', 'session', 'guise', 'severe', 'realism', 'eternal']\n",
      "\n",
      " Chapter 5: 6288 tokens\n",
      " First 10 tokens: ['symposium', 'lulled', 'mind', 'sleep', 'wisest', 'finest', 'hung', 'brilliant', 'curtain', 'doorway']\n",
      "\n",
      " Chapter 6: 4144 tokens\n",
      " First 10 tokens: ['broken', 'lute', 'seventhirty', 'august', 'living', 'wide', 'patiently', 'exchanging', 'tainted', 'inner']\n",
      "\n",
      " Chapter 7: 4318 tokens\n",
      " First 10 tokens: ['civilization', 'frantic', 'command', 'invisible', 'source', 'groped', 'thinking', 'remain', 'longer', 'away']\n",
      "\n",
      " Chapter 8: 4096 tokens\n",
      " First 10 tokens: ['aesthetic', 'camp', 'hooker', 'beautiful', 'gilberther', 'shell', 'lovely', 'bodymoved', 'broad', 'marble']\n",
      "\n",
      " Chapter 9: 3610 tokens\n",
      " First 10 tokens: ['become', 'player', 'lost', 'costume', 'lacking', 'pride', 'continue', 'note', 'tragedyso', 'hulme']\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Title: 'The Great Gatsby'\n",
      "\n",
      " Chapter 1: 1647 tokens\n",
      " First 10 tokens: ['younger', 'vulnerable', 'father', 'gave', 'advice', 'turning', 'mind', 'criticizing', 'anyone', 'remember']\n",
      "\n",
      " Chapter 2: 1130 tokens\n",
      " First 10 tokens: ['halfway', 'hastily', 'join', 'railroad', 'run', 'beside', 'quarter', 'mile', 'shrink', 'area']\n",
      "\n",
      " Chapter 3: 1721 tokens\n",
      " First 10 tokens: ['music', 'summer', 'garden', 'moth', 'whispering', 'champagne', 'star', 'tide', 'watched', 'guest']\n",
      "\n",
      " Chapter 4: 1470 tokens\n",
      " First 10 tokens: ['sunday', 'church', 'bell', 'rang', 'village', 'alongshore', 'mistress', 'returned', 'twinkled', 'hilariously']\n",
      "\n",
      " Chapter 5: 1138 tokens\n",
      " First 10 tokens: ['afraid', 'fire', 'peninsula', 'blazing', 'fell', 'unreal', 'shrubbery', 'elongating', 'glint', 'roadside']\n",
      "\n",
      " Chapter 6: 1095 tokens\n",
      " First 10 tokens: ['ambitious', 'reporter', 'arrived', 'inquired', 'politely', 'whyany', 'statement', 'transpired', 'confused', 'office']\n",
      "\n",
      " Chapter 7: 2236 tokens\n",
      " First 10 tokens: ['curiosity', 'highest', 'failed', 'saturday', 'obscurely', 'begun', 'career', 'trimalchio', 'gradually', 'become']\n",
      "\n",
      " Chapter 8: 1161 tokens\n",
      " First 10 tokens: ['sleep', 'foghorn', 'incessantly', 'tossed', 'halfsick', 'grotesque', 'reality', 'frightening', 'dream', 'dawn']\n",
      "\n",
      " Chapter 9: 1286 tokens\n",
      " First 10 tokens: ['remember', 'rest', 'endless', 'drill', 'police', 'photographer', 'newspaper', 'rope', 'stretched', 'main']\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"== Processed Book Summary ==\")\n",
    "print(f\"\\nTotal processed books: {len(book_chapter_tokens)}\")\n",
    "print(\"-\" * 40)\n",
    "for book, chapters in book_chapter_tokens.items():\n",
    "    print(f\"\\nTitle: '{book.replace('-', ' ').title()}'\\n\")\n",
    "    for i, chapter in enumerate(chapters):\n",
    "        print(f\" Chapter {i + 1}: {len(chapter['tokens'])} tokens\")\n",
    "        print(f\" First 10 tokens: {chapter['tokens'][:10]}\\n\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f468426",
   "metadata": {},
   "source": [
    "- **Phrase Detection**\n",
    "\n",
    "All chapters from the books are combined into a single list of documents, with each document represented as a list of tokens. Metadata for each chapter is stored separately for reference. The Gensim library is then used to identify common bigrams (two-word phrases) within the tokenized documents. Strict thresholds are applied to ensure that only meaningful, frequently co-occurring pairs are merged. Once identified, these bigrams replace the original word pairs in the documents with single tokens joined by an underscore (e.g., \"new_york\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25604626",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "chapter_metadata = []\n",
    "\n",
    "for book, chapters in book_chapter_tokens.items():\n",
    "    for chapter in chapters:\n",
    "        documents.append(chapter[\"tokens\"])\n",
    "        chapter_metadata.append({\n",
    "            \"book\": book,\n",
    "            \"chapter_title\": chapter[\"chapter_title\"]\n",
    "        })\n",
    "\n",
    "# Apply thresholds to avoid noisy bigrams\n",
    "bigram = Phrases(documents, min_count = 10, threshold = 20)\n",
    "bigram_mod = Phraser(bigram)\n",
    "\n",
    "documents = [bigram_mod[doc] for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25ba3f",
   "metadata": {},
   "source": [
    "- **Reviewing frequent words**\n",
    "\n",
    "The most frequent words in the corpus were analyzed, and those not relevant for topic modeling were added to the additional stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ecd66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 most common tokens:\n",
      "\n",
      "love: 191\n",
      "mind: 190\n",
      "half: 190\n",
      "away: 183\n",
      "boy: 126\n",
      "work: 126\n",
      "money: 125\n",
      "gave: 123\n",
      "god: 118\n",
      "friend: 112\n",
      "mother: 108\n",
      "party: 108\n",
      "heart: 106\n",
      "summer: 100\n",
      "kiss: 97\n",
      "beauty: 96\n",
      "child: 94\n",
      "together: 90\n",
      "city: 88\n",
      "best: 87\n",
      "laughed: 85\n",
      "war: 84\n",
      "hundred: 84\n",
      "beautiful: 83\n",
      "business: 83\n",
      "dream: 82\n",
      "letter: 82\n",
      "wife: 81\n",
      "lay: 81\n",
      "coming: 81\n",
      "brought: 80\n",
      "club: 80\n",
      "conversation: 79\n",
      "lot: 78\n",
      "drink: 78\n",
      "care: 77\n",
      "play: 77\n",
      "fell: 77\n",
      "married: 76\n",
      "dollar: 76\n",
      "step: 73\n",
      "lip: 72\n",
      "pretty: 72\n",
      "laughter: 71\n",
      "poor: 71\n",
      "yellow: 70\n",
      "sitting: 70\n",
      "crowd: 70\n",
      "broke: 70\n",
      "line: 69\n",
      "silence: 69\n",
      "reached: 69\n",
      "spring: 68\n",
      "live: 68\n",
      "smile: 68\n",
      "sent: 68\n",
      "dinner: 68\n",
      "sit: 68\n",
      "black: 68\n",
      "soul: 67\n",
      "lost: 67\n",
      "mouth: 66\n",
      "dance: 66\n",
      "body: 65\n",
      "slowly: 65\n",
      "present: 65\n",
      "shoulder: 65\n",
      "deep: 65\n",
      "dress: 64\n",
      "hard: 64\n",
      "finally: 64\n",
      "tree: 64\n",
      "making: 63\n",
      "keep: 62\n",
      "taken: 61\n",
      "coat: 61\n",
      "faint: 61\n",
      "shook: 61\n",
      "stand: 61\n",
      "sometimes: 61\n",
      "moon: 60\n",
      "talked: 60\n",
      "talking: 60\n",
      "desire: 60\n",
      "tired: 60\n",
      "known: 59\n",
      "effort: 59\n",
      "wait: 59\n",
      "quickly: 59\n",
      "broken: 59\n",
      "rest: 59\n",
      "rain: 59\n",
      "ten: 59\n",
      "try: 58\n",
      "thinking: 58\n",
      "held: 58\n",
      "cool: 58\n",
      "sleep: 58\n",
      "youth: 57\n",
      "country: 57\n"
     ]
    }
   ],
   "source": [
    "# Get most frequent words in the corpus\n",
    "def analyze_frequencies(texts, top_n):\n",
    "    \"\"\"\n",
    "    Analyze and print the top N most common tokens in the corpus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of list of str\n",
    "        Tokenized documents.\n",
    "    top_n : int\n",
    "        Number of top tokens to display.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    freq_dist : collections.Counter\n",
    "        Frequency distribution of all tokens in the corpus.\n",
    "    \"\"\"\n",
    "    # Flatten tokens from all documents\n",
    "    all_tokens = [token.lower() for doc in texts for token in doc]\n",
    "\n",
    "    # Count frequencies\n",
    "    freq_dist = Counter(all_tokens)\n",
    "\n",
    "    print(f\"Top {top_n} most common tokens:\\n\")\n",
    "    for token, count in freq_dist.most_common(top_n):\n",
    "        print(f\"{token}: {count}\")\n",
    "\n",
    "    return freq_dist\n",
    "\n",
    "freq_dist = analyze_frequencies(documents, top_n = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
